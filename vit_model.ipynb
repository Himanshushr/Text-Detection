{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# 1. CHECK GPU\n",
        "# ================================\n",
        "import torch\n",
        "print(\"GPU Available:\", torch.cuda.is_available())\n",
        "print(\"GPU Type:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\")\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n"
      ],
      "metadata": {
        "id": "pJziwFpju507",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66ca5547-f1f2-4d2f-8ae8-9ee50a07912b"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Available: True\n",
            "GPU Type: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gzm3SDKGlHEJ"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "zip_path = \"reduced_dataset.zip\"\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(\".\")\n",
        "\n",
        "print(\"Dataset extracted!\")\n",
        "print(os.listdir(\"reduced_dataset\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kl6TvS9yjfDZ",
        "outputId": "1c93a463-c765-4b7d-8a86-dd31187de2f8"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset extracted!\n",
            "['labels', 'images']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "print(\"Images:\", len(os.listdir(\"reduced_dataset/images\")))\n",
        "print(\"Labels:\", len(os.listdir(\"reduced_dataset/labels\")))\n"
      ],
      "metadata": {
        "id": "rmnTb6zYjfAL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89a52f3f-1959-4d40-c665-2ef5f7849282"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Images: 500\n",
            "Labels: 500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. INSTALL AND IMPORT DEPENDENCIES ---\n",
        "!pip install transformers torch pillow torchvision\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import ViTModel\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as T\n",
        "from torch.nn import CTCLoss\n",
        "import os\n",
        "from PIL import Image\n",
        "import random\n",
        "import string\n",
        "import numpy as np\n",
        "import sys"
      ],
      "metadata": {
        "id": "23AjISEjjek5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc42fd03-3a9e-4f0d-a011-88368f650118"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.40.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (11.3.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.24.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (1.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 2. DATASET PATHS ---\n",
        "# !!! IMPORTANT !!!\n",
        "# Please ensure you have uploaded your dataset to these directories\n",
        "# (e.g., recognition_dataset/images/001.png and recognition_dataset/labels/001.txt)\n",
        "\n",
        "IMAGE_DIR = \"reduced_dataset/images\"\n",
        "LABEL_DIR = \"reduced_dataset/labels\"\n",
        "NUM_EPOCHS = 2 # Reduced number of epochs for quick demonstration\n",
        "BATCH_SIZE = 4\n",
        "\n",
        "# Check if data directories exist\n",
        "if not os.path.exists(IMAGE_DIR) or not os.path.exists(LABEL_DIR):\n",
        "    print(\"--- ⚠️ DATA WARNING ⚠️ ---\")\n",
        "    print(f\"Image directory not found: {IMAGE_DIR}\")\n",
        "    print(\"Please upload your 'recognition_dataset' folder containing 'images' and 'labels' to the Colab environment.\")\n",
        "    sys.exit(1)\n",
        "\n",
        "if not os.listdir(IMAGE_DIR):\n",
        "    print(\"--- ⚠️ DATA WARNING ⚠️ ---\")\n",
        "    print(f\"Image directory {IMAGE_DIR} is empty.\")\n",
        "    print(\"Please ensure your dataset files (.png and .txt) are uploaded.\")\n",
        "    sys.exit(1)\n",
        "print(f\"Found {len(os.listdir(IMAGE_DIR))} image files. Proceeding with training...\")"
      ],
      "metadata": {
        "id": "h-we64wYjedJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72b30b3c-d8cb-4e1d-caa1-96466b63a075"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 500 image files. Proceeding with training...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 3. MODEL DEFINITION ---\n",
        "\n",
        "class ViT_CTC_Model(nn.Module):\n",
        "    \"\"\"\n",
        "    ViT-based model for sequence recognition using CTC loss.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes):\n",
        "        super().__init__()\n",
        "        # Load pre-trained Vision Transformer\n",
        "        self.vit = ViTModel.from_pretrained(\"google/vit-base-patch16-224\")\n",
        "\n",
        "        # Linear layer for CTC output. ViT-base output hidden size is 768.\n",
        "        self.fc = nn.Linear(768, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        outputs = self.vit(pixel_values=x)\n",
        "        # last_hidden_state: (B, sequence_length=197, hidden_size=768)\n",
        "        x = outputs.last_hidden_state\n",
        "        x = self.fc(x) # Output shape: (B, seq, num_classes)\n",
        "        return x"
      ],
      "metadata": {
        "id": "MkdwgTQluiLL"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 4. DATASET DEFINITION ---\n",
        "\n",
        "class RecognitionDataset(Dataset):\n",
        "    \"\"\"Custom Dataset for loading images and text labels.\"\"\"\n",
        "    def __init__(self, image_dir, label_dir, transforms=None):\n",
        "        self.image_dir = image_dir\n",
        "        self.label_dir = label_dir\n",
        "        self.image_files = sorted(os.listdir(image_dir))\n",
        "        # Default transforms for ViT input\n",
        "        self.transforms = transforms if transforms else T.Compose([\n",
        "            T.Resize((224, 224)),\n",
        "            T.ToTensor(),\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.image_files[idx]\n",
        "        img_path = os.path.join(self.image_dir, img_name)\n",
        "\n",
        "        # Determine label file name\n",
        "        txt_name = img_name.split('.')[0] + \".txt\"\n",
        "        label_path = os.path.join(self.label_dir, txt_name)\n",
        "\n",
        "        # load image\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        img = self.transforms(img)\n",
        "\n",
        "        # load label (text)\n",
        "        with open(label_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            text = f.read().strip()\n",
        "\n",
        "        return img, text"
      ],
      "metadata": {
        "id": "xTY0gg6nucI4"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 5. TRAINING SCRIPT ---\n",
        "\n",
        "# Character set definition (must include all possible characters in your labels)\n",
        "charset = \"0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
        "# Mapping characters to indices (1-based, 0 is reserved for the CTC blank token)\n",
        "char2idx = {c:i+1 for i,c in enumerate(charset)}\n",
        "char2idx[\"<blank>\"] = 0\n",
        "num_classes = len(char2idx) # Final output dimension\n",
        "\n",
        "# Function to convert a text string into a tensor of indices\n",
        "def encode_text(text):\n",
        "    return torch.tensor([char2idx[c] for c in text if c in char2idx], dtype=torch.long)"
      ],
      "metadata": {
        "id": "qxh7ShscukTE"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dataset and dataloader\n",
        "train_dataset = RecognitionDataset(IMAGE_DIR, LABEL_DIR)\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)"
      ],
      "metadata": {
        "id": "qt0ie3uBukIT"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Model, Optimizer, and Loss\n",
        "model = ViT_CTC_Model(num_classes)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "# blank=0 is crucial\n",
        "criterion = CTCLoss(blank=0, reduction='mean')\n",
        "\n",
        "# Setup device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vPXEuY69ukBF",
        "outputId": "2126b26b-01cb-4d98-ee93-db94ca95711d"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ViT_CTC_Model(\n",
              "  (vit): ViTModel(\n",
              "    (embeddings): ViTEmbeddings(\n",
              "      (patch_embeddings): ViTPatchEmbeddings(\n",
              "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
              "      )\n",
              "      (dropout): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (encoder): ViTEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x ViTLayer(\n",
              "          (attention): ViTAttention(\n",
              "            (attention): ViTSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (output): ViTSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): ViTIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): ViTOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    (pooler): ViTPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (fc): Linear(in_features=768, out_features=63, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Training on device: {device}\")\n",
        "print(\"Training started...\")\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    total_loss = 0\n",
        "    model.train()\n",
        "    for imgs, texts in train_loader:\n",
        "        imgs = imgs.to(device)\n",
        "        labels_list = [encode_text(t) for t in texts]\n",
        "\n",
        "        # 1. Target labels (concatenated)\n",
        "        labels = torch.cat(labels_list).to(device)\n",
        "        # 2. Lengths of target labels before concatenation\n",
        "        label_lens = torch.tensor([len(l) for l in labels_list], dtype=torch.long)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(imgs)  # (B, seq, num_classes)\n",
        "\n",
        "        # 3. Permute output: CTC requires (sequence_length, Batch_size, num_classes)\n",
        "        outputs = outputs.permute(1, 0, 2)\n",
        "\n",
        "        # 4. Lengths of input sequence: full length of the ViT's patch sequence (197)\n",
        "        input_lens = torch.full(\n",
        "            size=(outputs.size(1),),\n",
        "            fill_value=outputs.size(0),\n",
        "            dtype=torch.long\n",
        "        )\n",
        "\n",
        "        # Calculate CTC Loss\n",
        "        loss = criterion(outputs, labels, input_lens, label_lens)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS}: Loss = {total_loss / len(train_loader):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UABow640ujwc",
        "outputId": "8f0727c6-7a5d-4600-f16a-9a6393962615"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on device: cuda\n",
            "Training started...\n",
            "Epoch 1/2: Loss = 2.8369\n",
            "Epoch 2/2: Loss = 2.9975\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the trained model\n",
        "MODEL_PATH = \"vit_ctc.pth\"\n",
        "torch.save(model.state_dict(), MODEL_PATH)\n",
        "print(f\"Training complete! Model saved as {MODEL_PATH}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EqyD518_vIHz",
        "outputId": "75e8cc65-14ec-41cb-f4d5-66c2cf610d15"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training complete! Model saved as vit_ctc.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "49GCdJcevkZu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}